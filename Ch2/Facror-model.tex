\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}

\title{Factor model}

\author{
    Xie zejian
   \\
     \\
   \\
  \texttt{\href{mailto:xiezej@gmail.com}{\nolinkurl{xiezej@gmail.com}}} \\
  }

\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newenvironment{cslreferences}%
  {\setlength{\parindent}{0pt}%
  \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces}%
  {\par}

\begin{document}
\maketitle

\def\tightlist{}


\begin{abstract}

\end{abstract}


\hypertarget{capm}{%
\section{CAPM}\label{capm}}

\hypertarget{beta-representation}{%
\subsection{Beta representation}\label{beta-representation}}

Recall the tangency portfolio is
\(\omega_D=\frac{\mathbf{V^-}(\overline{\mathbf{r}}-r_f\mathbf{e})}{\mathbf{e'V^-}(\overline{\mathbf{r}}-r_f\mathbf{e})}\).
Write \(\omega_D=m \mathbf{V^-} (\overline{\mathbf{r}}-r_f\mathbf{e})\)
where
\(m=\frac{1}{\mathbf{e'}\mathbf{V^-} (\overline{\mathbf{r}}-r_f\mathbf{e})}\),
then we have

\[ \overline{r}-r_f\mathbf{e}=\frac{1}{m}\mathbf{V}\omega_D \]

Note \(\cov(\mathbf{r},\omega'\mathbf{r})=\mathbf{V\omega}\) and

\[ \sigma_D^2=\omega_D'\mathbf{V}\omega_D=m\omega'_D(\overline{\mathbf{r}}-r_f\mathbf{e})= mr_D-mr_f \]

we have

\[ \overline{\mathbf{r}}-r_f\mathbf{e}=\frac{r_D-r_f}{\sigma_D^2}\cov(\mathbf{r},{r_D}) \]

Denote \(\frac{\cov(\mathbf{r},{r_D})}{\sigma_D^2}=\beta_D\), we have

\[ \overline{\mathbf{r}}-r_f\mathbf{e}=\beta_D({r_D-r_f}) \]

Similar results also holds for any portfolio \(\overline{r}_p\) in the
MVF:

\[ \overline{\mathbf{r}}-\overline{r}_q\mathbf{e}=\beta_p(\overline{r}_p-\overline{r}_q) \]

It's clear in the view of every portfolio \(\overline{r}_p\) is also a
tangency portfolio by selecting proper \(r_f\). One can also check it in
a dirty way:

\textbf{Proof} Suppose \(r_p\) and \(r_q\) both in the MVF without
risk-free asset, recall

\[ \begin{aligned}
  \omega_p'\mathbf{V}\omega_q=\frac{1}{\delta}+\frac{\delta(\overline{r}_p-\frac{\alpha}{\delta})(\overline{r}_q-\frac{\alpha}{\delta})}{\delta\xi-\alpha^2}
\end{aligned} \]

If the covariance is \(0\), we have

\[ \overline{r}_q=\frac{\alpha}{\delta}-\frac{\delta\xi-\alpha^2}{\delta^2(\overline{r}_p-\alpha/\delta)} \]

Then

\[ \begin{aligned}
  \mathbf{\overline{r}}-\overline{r}_q \mathbf{e}&=\mathbf{\overline{r}}-(\frac{\alpha}{\delta}-\frac{\delta\xi-\alpha^2}{\delta^2(\overline{r}_p-\alpha/\delta)})\mathbf{e}
  \\&=\frac{1}{\delta^2(\overline{r}_p-\alpha/\delta)}(\delta^2(\overline{r}_p-\alpha/\delta))(\mathbf{\overline{r}}-(\frac{\alpha}{\delta}-\frac{\delta\xi-\alpha^2}{\delta^2(\overline{r}_p-\alpha/\delta)})\mathbf{e})
  \\&=\frac{1}{\delta^2(\overline{r}_p-\alpha/\delta)}(\mathbf{\overline{r}}(\delta^2(\overline{r}_p-\alpha/\delta))-(\alpha\delta(\overline{r}_p-\alpha/\delta)-(\delta\xi-\alpha^2))\mathbf{e})
  \\&=\frac{1}{\delta^2(\overline{r}_p-\alpha/\delta)}(\mathbf{\overline{r}}(\delta^2(\overline{r}_p-\alpha/\delta))-(\alpha\delta\overline{r}_p-\delta\xi)\mathbf{e}
  \\&=\frac{
    (\delta^2\overline{r}_p\mathbf{\overline{r}}-\alpha\delta\mathbf{\overline{r}})-(\alpha\delta\overline{r}_p-\delta\xi)\mathbf{e}}
  {\delta^2(\overline{r}_p-\alpha/\delta)}
  \\&=\frac{
    (\delta\overline{r}_p-\alpha)\mathbf{\overline{r}}-(\alpha\overline{r}_p-\xi)\mathbf{e}}
  {\delta(\overline{r}_p-\alpha/\delta)}
\end{aligned} \]

On the other hand:

\[ \begin{aligned}
  \beta_p&=\frac{\mathbf{V\omega_p}}{\omega_p'\mathbf{V}\omega_p}
  \\&=\frac{1}{\omega_p'\mathbf{V}\omega_p}(\lambda_p\overline{\mathbf{r}}+\gamma \mathbf{e})
  \\&=\frac{1}{\omega_p'\mathbf{V}\omega_p}(\frac{\mathbf{\xi e-\alpha  \overline{r}}}{\delta\xi-\alpha^2}+\frac{\mathbf{-\alpha e+\delta\overline{r}}}{\delta\xi-\alpha^2}\overline{r}_p)
  \\&=\frac{1}{\omega_p'\mathbf{V}\omega_p}(\frac{ (\delta\overline{r}_p-\alpha)\mathbf{\overline{r}}-(\alpha\overline{r}_p-\xi)\mathbf{e}}{\Delta})
\end{aligned} \]

Then it's remain to show that

\[ (\overline{r}_p-\overline{r}_q)\delta(\overline{r}_p-\alpha/\delta)=\omega'\mathbf{V}\omega\Delta \]

It's clear since

\[ \omega'\mathbf{V}\omega \Delta=\sigma_p^2\Delta=\frac{\Delta}{\delta}+\delta(\overline{r}_p-\frac{\alpha}{\delta})^2 \]

and

\[ \begin{aligned}
  (\overline{r}_p-\overline{r}_q)\delta(\overline{r}_p-\alpha/\delta)&=
  ((\overline{r}_p-\frac{\alpha}{\delta})+\frac{\delta\xi-\alpha^2}{\delta^2(\overline{r}_p-\alpha/\delta)})\delta(\overline{r}_p-\alpha/\delta)
  \\&=\frac{\Delta}{\delta}+\delta(\overline{r}_p-\frac{\alpha}{\delta})^2
\end{aligned} \]

\hypertarget{capm-1}{%
\subsection{CAPM}\label{capm-1}}

In capital market equilibrium, the market portfolio is tangecy portfolio
\(r_D={r}_m\), then

\[ \overline{\mathbf{r}}-r_f\mathbf{e}=\beta_m({{r}_m-r_f}) \]

where

\[ \beta_m=\begin{bmatrix}
  \frac{\cov(r_1,r_m)}{\sigma^2_m}\\
  \frac{\cov(r_2,r_m)}{\sigma^2_m}\\
  \cdots\\
  \frac{\cov(r_n,r_m)}{\sigma^2_m}\\
\end{bmatrix} \]

this equation is called \textbf{Sharpe-Lintner CAPM}. \(r_m-r_f\) is
called \textbf{market risk premium} and \(\frac{r_m-r_f}{\sigma_m}\) is
called \textbf{market sharpe ratio}. Translate it from vector form, we
get the \textbf{Security Market Line}:

\[ 
r_i-r_f=\beta_{i,m}(r_m-r_f) 
\]

\hypertarget{realized-return}{%
\subsubsection{Realized return}\label{realized-return}}

Now consider both \(r_i\) and \(r_m\) is random variable, let
\(\epsilon\) be a random vector with zero expection and zero covariance
with \(r_i\) and \(r_m\), then

\[ 
r_i-r_f=\beta_{i,m}(r_m-r_f)+\epsilon_i
\]

This is a regression equation, if one include an intercept, then the
model

\[ 
r_i-r_f=\alpha_i+\beta_{i,m}(r_m-r_f)+\epsilon_i
\]

is called \textbf{market model}, such \(\alpha\) is called
\textbf{Jensen's alpha}.

\hypertarget{variance-decomposition}{%
\subsubsection{Variance decomposition}\label{variance-decomposition}}

Decomposition the variance as:

\[ \var(r_i)=\overbrace{\underbrace{\beta_i\sigma_m^2}_{\text{Systematic risk}}+\underbrace{\var(\epsilon_i)}_{\text{Idiosyncratic risk}}}^{\text{total risk}} \]

The \(R^2\) is just the proportion of systematic risk

\[ R^2=\frac{\beta_i^2\sigma_m^2}{\beta_i^2\sigma_m^2+\sigma^2} \]

since
\href{https://en.wikipedia.org/wiki/Fraction_of_variance_unexplained}{Fraction
of variance unexplained}.

\hypertarget{testing-capm}{%
\subsubsection{Testing CAPM}\label{testing-capm}}

Suppose we run time series regressions for each of the \(n\) risky
assets

\[ \mathbf{r_t^e=\alpha}+{\beta r_{m,t}^e+\nu_t} \]

where \(\alpha,\mathbf{r_t^e},\beta,\nu_t\) are \(n\times 1\) vector and
\(r_{m,t}^e\) is scalar.

By the discussion above, \(\alpha=\mathbf{0}\) when CAPM holds. Assume
\(\{\nu_t\}_{t=1}^T\) i.i.d with \(\mathcal{N}(0,\Sigma)\), we have
\(\mathbf{r_t^e}\mid r_{m,t}^e\sim \mathcal{N}(\alpha+\beta r_{m,t}^e,\Sigma)\).

Suppose

\[\mathbf{r}= \begin{bmatrix}
  \mathbf{r_1^e}'\\
  \mathbf{r_2^e}'\\
  \mathbf{r_3^e}'\\
  \vdots\\
  \mathbf{r_T^e}'
\end{bmatrix},\mathbf{r_m}=\begin{bmatrix}
  r_{m,1}^e\\
  r_{m,2}^e\\
  r_{m,3}^e\\
  \vdots\\
  r_{m,T}^e
\end{bmatrix} \]

Now
\(\mathbf{r'}\sim \mathcal{MN}_{n\times T}(\mathbf{\alpha e'+\beta r_m',\Sigma,I})\),
the p.d.f is (Wikipedia contributors 2019)

\[ p(\mathbf{r'|\beta r_m',\Sigma,I})=\frac{\exp(-\frac{1}{2} \tr[\mathbf{(r'-\alpha e'-\beta r_m')'\Sigma^-(r'-\alpha e'-\beta r_m')}])}{(2\pi)^{nT/2}T^{n/2}|\mathbf{\Sigma}|^{T/2}} \]

thus the log likelihood function is

\[ \log L=-\frac{1}{2} \tr[\mathbf{(r'-\alpha e'-\beta r_m')'\Sigma^-(r'-\alpha e'-\beta r_m')}]-\frac{nT}{2}\log 2\pi-\frac{n}{2}\log T-\frac{T}{2}\log |\Sigma| \]

FOC w.r.t \(\alpha\), by chain rule(Petersen and Pedersen 2012)

\[ \begin{aligned}
  \partial \log L&=\tr\left(\frac{\partial \log L}{\partial \mathbf{(r'-\alpha e'-\beta r_m')}}'\partial\mathbf{(X-\alpha e'-\beta r_m')}\right)
  \\&=\tr\left(\frac{\partial \log L}{\partial \mathbf{(r'-\alpha e'-\beta r_m')}}'\partial\alpha \mathbf{(-e')}\right)
  \\&=\tr\left(\mathbf{(-e')}\frac{\partial \log L}{\partial \mathbf{(r'-\alpha e'-\beta r_m')}}'\partial\alpha\right)
\end{aligned} \]

hence

\[ \begin{aligned}
  \frac{\partial \log L}{\partial \alpha}&=-\frac{\partial \log L}{\partial \mathbf{(r'-\alpha e'-\beta r_m')}}\mathbf{e}
  \\&=-(\Sigma^-+\Sigma'^-)\mathbf{(r'-\alpha e'-\beta r_m')}\mathbf{e}=0
\end{aligned} \]

Similarly, FOC w.r.t \(\beta\) and combine those results:

\[ \begin{aligned}
  \mathbf{(r'-\hat{\alpha}e'-\hat{\beta}r_m')r_m=0}\\
  \mathbf{(r'-\hat{\alpha}e'-\hat{\beta}r_m')e=0}
\end{aligned} \]

which leads to a linear equation

\[ \begin{bmatrix}
  \mathbf{e'r_m}&\mathbf{r_m'r_m}\\
  \mathbf{e'e}&\mathbf{r_m'e}
\end{bmatrix}\begin{bmatrix}
  \hat{\alpha}\\\hat{\beta}
\end{bmatrix}=\begin{bmatrix}
  \mathbf{r'r_m}\\
  \mathbf{r'e}
\end{bmatrix} \]

Similarly to out deduction for mean-variance mdoel, let
\(a=\mathbf{r_m'r_m},b=\mathbf{e'e}=T\) and
\(c=\mathbf{e'r_m}\)(\(c^2< ab\)), hence

\[ \begin{cases}
  \hat{\alpha}=c\mathbf{r'r_m}-a\mathbf{r'e}/(c^2-ab)\\
  \hat{\beta}=-b\mathbf{r'r_m}+c\mathbf{r'e}/(c^2-ab)
\end{cases} \]

By assumption
\(\mathbf{r'}\sim \mathcal{MN}_{n\times T}(\mathbf{\beta r_m',\Sigma,I})\),
and \(\alpha=\mathbf{r'}(c\mathbf{r_m}-a\mathbf{e})/(c^2-ab)\)

By transformation of matrix normal distribution(Wikipedia contributors
2019)

\[\frac{ (c\mathbf{r_m}-a\mathbf{e})'(c\mathbf{r_m}-a\mathbf{e})}{(c^2-ab)^2}=\frac{c^2a-2ac^2+a^2b}{(c^2-ab)^2}=\frac{a}{ab-c^2} \]

we have

\[ \hat{\alpha}\sim \mathcal{MN}(\mathbf{0,\Sigma},\frac{a}{ab-c^2}) \]

which degenerated to \(\mathcal{N}(0,\mathbf{\frac{a}{ab-c^2}\Sigma})\)
since
\(\Sigma \otimes \frac{a}{ab-c^2}=\frac{a}{ab-c^2}\Sigma\).(Wikipedia
contributors 2019) For the same reason,
\(\hat{\beta}\sim\mathcal{N}(\mathbf{\beta},\frac{b}{ab-c^2}\Sigma)\)

FOC w.r.t \(\Sigma\)(Petersen and Pedersen 2012):

\[ \frac{\partial \log L}{\partial \Sigma}=\frac{1}{2}(\Sigma^-\mathbf{(r'-\alpha e'-\beta r_m')}\mathbf{(r'-\alpha e'-\beta r_m')}'\Sigma^-)'-\frac{T}{2}\Sigma'^-=0 \]

hence

\[ \hat{\Sigma}= \mathbf{(r'-\hat{\alpha} e'-\hat{\beta} r_m')}\mathbf{(r'-\hat{\alpha} e'-\hat{\beta} r_m')}'/T\]

Where

\[ \mathbf{(r'-\hat{\alpha} e'-\hat{\beta} r_m')}=\mathbf{r'}(\mathbf{I}-\frac{c\mathbf{r_me'}-a\mathbf{ee'}-b\mathbf{r_mr_m'}+c\mathbf{er_m'}}{c^2-ab}) \]

Easy to verify
\(\frac{c\mathbf{r_me'}-a\mathbf{ee'}-b\mathbf{r_mr_m'}+c\mathbf{er_m'}}{c^2-ab}\)
is symmetric and idempotent, thus

\[ \begin{aligned}
  \rank(\mathbf{I}-\frac{c\mathbf{r_me'}-a\mathbf{ee'}-b\mathbf{r_mr_m'}+c\mathbf{er_m'}}{c^2-ab})&=\tr(\mathbf{I}-\frac{c\mathbf{r_me'}-a\mathbf{ee'}-b\mathbf{r_mr_m'}+c\mathbf{er_m'}}{c^2-ab})
  \\&=T-\frac{2c^2-2ab}{c^2-ab}
  \\&=T-2
\end{aligned} \]

By following lemma:

\begin{quote}
Suppose symmetric matrix \(p\times p\) \(\mathbf{A}\). It's idempotent
of rank \(s\) iff there exist a \(p\times s\) \(\mathbf{P}\)
\(\ni \mathbf{PP'=A}\) and \(\mathbf{P'P=I}\).
\end{quote}

\textbf{Proof} Sufficiency is trivial. For necessity, since
\(\mathbf{A}\) is symmetric and idempotent matrix, it can be spectral
decompostioned by \(\mathbf{A=Q\Lambda Q'}\). Where the diagonal of
\(\mathbf{\Lambda}\) is \(s\) 1 and \(p-s\) 0. Thus

\[ \mathbf{A=Q\Lambda Q'}= \left( \begin{array} { l l } \mathbf{P_1} & \mathbf{P_2} \end{array} \right) \left( \begin{array} { l l } \mathbf { I } _ { s } & 0 \\ 0 & 0 \end{array} \right) \left( \begin{array} { l } \mathbf { P } _ { 1 } ^ { \prime } \\ \mathbf { P } _ { 2 } ^ { \prime } \end{array} \right) = \mathbf { P } _ { 1 } \mathbf { P } _ { 1 } ^ { \prime } \]

Note

\[ \mathbf { I } _ { p } = \mathbf { Q } ^ { \prime } \mathbf { Q } = \left( \begin{array} { c } \mathbf { P } _ { 1 } ^ { \prime } \\ \mathbf { P } _ { 2 } ^ { \prime } \end{array} \right) \left( \begin{array} { c c } \mathbf { P } _ { 1 } & \mathbf { P } _ { 2 } \end{array} \right) = \left( \begin{array} { c c } \mathbf { P } _ { 1 } ^ { \prime } \mathbf { P } _ { 1 } & \mathbf { P } _ { 1 } ^ { \prime } \mathbf { P } _ { 2 } \\ \mathbf { P } _ { 2 } ^ { \prime } \mathbf { P } _ { 1 } & \mathbf { P } _ { 2 } ^ { \prime } \mathbf { P } _ { 2 } \end{array} \right)= \left( \begin{array} { c c } \mathbf { P } _ { 1 } ^ { \prime } \mathbf { P } _ { 1 } & \mathbf{0}\\ \mathbf{0} & \mathbf { P } _ { 2 } ^ { \prime } \mathbf { P } _ { 2 } \end{array} \right)\]

hence\(\mathbf{P_1'P_1=I_s}.\blacksquare\)

We may find
\(\mathbf{r'P}\sim \mathcal{MN}_{n\times(T-2)}(\mathbf{0},\Sigma,\mathbf{I})\).
Where
\(\ep[\mathbf{r'P}]=\ep[\mathbf{r'PP'P}]=\ep[\mathbf{r'AP}]=\mathbf{0}\)
and thus (Wikipedia contributors 2020)

\[ T\hat{\Sigma}=\mathbf{r'Ar}=\mathbf{r'PP'r}=\mathbf{r'P(r'P)'}\sim W_n(T-2,\Sigma) \]

\newpage

\hypertarget{refs}{}
\begin{cslreferences}
\leavevmode\hypertarget{ref-petersen2012matrix}{}%
Petersen, Kaare Brandt, and Michael Syskind Pedersen. 2012. ``The Matrix
Cookbook, Nov 2012.'' \emph{URL Http://Www2. Imm. Dtu. Dk/Pubdb/P. Php}
3274: 14.

\leavevmode\hypertarget{ref-wiki:mnt}{}%
Wikipedia contributors. 2019. ``Matrix Normal Distribution ---
Wikipedia, the Free Encyclopedia.''
\url{https://en.wikipedia.org/w/index.php?title=Matrix_normal_distribution\&oldid=902125596}.

\leavevmode\hypertarget{ref-wiki:wst}{}%
---------. 2020. ``Wishart Distribution --- Wikipedia, the Free
Encyclopedia.''
\url{https://en.wikipedia.org/w/index.php?title=Wishart_distribution\&oldid=986003757}.
\end{cslreferences}

\bibliographystyle{unsrt}
\bibliography{references.bib}


\end{document}
